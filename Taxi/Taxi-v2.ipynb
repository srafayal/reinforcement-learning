{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from gym import envs\n",
    "import gym\n",
    "import random\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(envs.registry.all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-05-14 11:55:25,163] Making new env: Taxi-v2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\smr99\\anaconda3\\envs\\ps\\lib\\site-packages\\gym\\envs\\registration.py:17: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n"
     ]
    }
   ],
   "source": [
    "#https://github.com/openai/gym/blob/master/gym/envs/toy_text/taxi.py\n",
    "#https://medium.com/@anirbans17/reinforcement-learning-for-taxi-v2-edd7c5b76869\n",
    "env = gym.make('Taxi-v2')\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# State Space\n",
    "\n",
    "|Information| Possible Values | Number of possible Values|\n",
    "|-----------|-----------------|--------------------------|\n",
    "| Taxi Row  |0,1,2,3,4        | 5                        |\n",
    "| Taxi Column  |0,1,2,3,4     | 5                        |\n",
    "| Destination  |R,G,B,Y (0,1,2,3)      | 4                        |\n",
    "|Passenger location  |R,G,B,Y, Onboard       | 5         |\n",
    "\n",
    "# Action Space\n",
    "\n",
    "- 0: going down (south)\n",
    "- 1: going up (north)\n",
    "- 2: going right (east)\n",
    "- 3: going left (west)\n",
    "- 4: picking up\n",
    "- 5: dropping off\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action_size:6\n",
      "state_size:500\n"
     ]
    }
   ],
   "source": [
    "action_size = env.action_space.n\n",
    "print('action_size:{}'.format(action_size))\n",
    "\n",
    "state_size = env.observation_space.n\n",
    "print('state_size:{}'.format(state_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.action_space \n",
    "num_episodes=50000\n",
    "test_episodes = 100\n",
    "steps = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each state of our agent, which is represented by a vector of values [row of taxi, col of taxi, passenger index, destination index], is consequently encoded with a value between 0 and 499. Namely, we can replicate the location of our previous picture like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State: 247\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : : : : |\n",
      "| : :\u001b[43m \u001b[0m: : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "env = env.unwrapped\n",
    "state = env.encode(2, 2, 1, 3) \n",
    "print(\"State:\", state)\n",
    "env.s = state\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "decode_states=[]\n",
    "for i in range(500):\n",
    "    decode_states.append(list(env.decode (i) ))   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, knowing that our taxi is in position (4,2) and that passenger’s index=3 and drop-off location=2, we can derive that the encoded state is 454. For the following experiment, we will use this starting point, but before diving into that, we need to introduce one last element: the rewarding system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rewarding system\n",
    "    There is a default per-step reward of -1,\n",
    "    except for delivering the passenger, which is +20,\n",
    "    or executing \"pickup\" and \"drop-off\" actions illegally, which is -10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: [(1.0, 454, -1, False)],\n",
       " 1: [(1.0, 354, -1, False)],\n",
       " 2: [(1.0, 454, -1, False)],\n",
       " 3: [(1.0, 434, -1, False)],\n",
       " 4: [(1.0, 454, -10, False)],\n",
       " 5: [(1.0, 454, -10, False)]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# {action: [(probability, nextstate, reward, done)]\n",
    "env.P[454]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monte Carlo Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents import mc_off_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0/1Elapsed time: 1.609375 1.5\n",
      "Elapsed time during the whole program in seconds: 0.109375\n"
     ]
    }
   ],
   "source": [
    "from time import process_time \n",
    "from IPython.display import clear_output\n",
    "\n",
    "t1_start = process_time() \n",
    "#num_episodes\n",
    "agent = mc_off_policy.MonteCarloAgent(state_size, action_size,1, env)\n",
    "agent.initialize_state_actions() \n",
    "agent.control()\n",
    "\n",
    "t1_stop = process_time() \n",
    "   \n",
    "print(\"Elapsed time:\", t1_stop, t1_start) \n",
    "print(\"Elapsed time during the whole program in seconds:\", \n",
    "                                         t1_stop-t1_start) \n",
    "\n",
    "MonteCarloAgent_Q= agent.Q_vals\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.save('model_output/final_MC_off_vals_50K.pkl')\n",
    "\n",
    "agent.load('model_output/final_MC_off_vals_50K.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "# env.reset()\n",
    "# rewards =[]\n",
    "\n",
    "# for e in range(3):\n",
    "#     done = False\n",
    "#     state = env.reset()\n",
    "#     s = 0\n",
    "#     r=0\n",
    "#     while not done:\n",
    "#         #env.render()\n",
    "         \n",
    "#         action = np.argmax(list(MonteCarloAgent_Q[state].values()))\n",
    "#         new_state, reward, done, info = env.step(action)\n",
    "#         r+=reward\n",
    "#         state = new_state\n",
    "\n",
    "#         if done:\n",
    "#             rewards.append(r)\n",
    "#             break\n",
    "# env.close() \n",
    "# MC_rewards=rewards\n",
    "# print('the score is: {}'.format(sum(rewards)/test_episodes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Qlearning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents import qlearning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0/4.***************************\n",
      "292\n",
      "***************************\n",
      "***************************\n",
      "112\n",
      "***************************\n",
      "***************************\n",
      "92\n",
      "***************************\n",
      "***************************\n",
      "384\n",
      "***************************\n",
      "Elapsed time: 1.734375 1.5625\n",
      "Elapsed time during the whole program in seconds: 0.171875\n"
     ]
    }
   ],
   "source": [
    "from time import process_time \n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Start the stopwatch / counter  \n",
    "t1_start = process_time() \n",
    "num_episodes=4\n",
    "agent = qlearning.QLearning(state_size, action_size, env)\n",
    "agent.initialize_state_actions() \n",
    " \n",
    "for i_episode in range(num_episodes):\n",
    "    if i_episode % 100 == 0:\n",
    "        print(\"\\rEpisode {}/{}.\".format(i_episode, num_episodes), end=\"\")\n",
    "        sys.stdout.flush()\n",
    "    agent.control()\n",
    "\n",
    "    \n",
    "t1_stop = process_time() \n",
    "   \n",
    "print(\"Elapsed time:\", t1_stop, t1_start) \n",
    "print(\"Elapsed time during the whole program in seconds:\", \n",
    "                                         t1_stop-t1_start) \n",
    "\n",
    "QLearning_Q= agent.Q_vals\n",
    "#     if i%10 == 9:\n",
    "#         mcc.evaluate_target_policy()\n",
    "    \n",
    "#     if i%100 == 99:\n",
    "#         mcc.save_your_work()\n",
    "#         mcc.plot_rewards()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.save('model_output/final_QLearning_50K.pkl')\n",
    "\n",
    "agent.load('model_output/final_QLearning_50K.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SARSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents import sarsa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 49999/50000. ...0.10001800036000719Elapsed time: 508.34375 1.3125\n",
      "Elapsed time during the whole program in seconds: 507.03125\n"
     ]
    }
   ],
   "source": [
    "from time import process_time \n",
    "from IPython.display import clear_output\n",
    "num_episodes=50000\n",
    "# Start the stopwatch / counter  \n",
    "t1_start = process_time() \n",
    "\n",
    "agent = sarsa.Sarsa(state_size, action_size,num_episodes, env)\n",
    "agent.initialize_state_actions( )\n",
    "agent.control() \n",
    " \n",
    "# for i_episode in range(num_episodes):\n",
    "#     if i_episode % 100 == 0:\n",
    "#         print(\"\\rEpisode {}/{}.\".format(i_episode, num_episodes), end=\"\")\n",
    "#         sys.stdout.flush()\n",
    "    \n",
    "\n",
    "    \n",
    "t1_stop = process_time() \n",
    "   \n",
    "print(\"Elapsed time:\", t1_stop, t1_start) \n",
    "print(\"Elapsed time during the whole program in seconds:\", \n",
    "                                         t1_stop-t1_start)\n",
    "\n",
    "Sarsa_Q= agent.Q_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.save('model_output/final_sarsa_50K.pkl')\n",
    "\n",
    "agent.load('model_output/final_sarsa_50K.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DynaQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents import dynaq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0/50000."
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-6772997a105e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\rEpisode {}/{}.\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi_episode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_episodes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m     \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\PhD\\Git\\Syed_progress\\exercises\\Taxi\\agents\\dynaq.py\u001b[0m in \u001b[0;36mcontrol\u001b[1;34m(self, γ)\u001b[0m\n\u001b[0;32m    144\u001b[0m                 \u001b[1;31m#Q(S,A)<-Q(S,A)+ α[R + γ* max_a Q(S',a)-Q(S,A)]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    145\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 146\u001b[1;33m                 \u001b[0mbest_next_action_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mQ_vals\u001b[0m\u001b[1;33m[\u001b[0m \u001b[0mnext_state_\u001b[0m \u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    147\u001b[0m                 \u001b[0mtarget_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreward_\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mγ\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mQ_vals\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnext_state_\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbest_next_action_\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    148\u001b[0m                 \u001b[0mqUpdate_\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0mtarget_\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mQ_vals\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstate_\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0maction_\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36margmax\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\pd\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36margmax\u001b[1;34m(a, axis, out)\u001b[0m\n\u001b[0;32m   1184\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1185\u001b[0m     \"\"\"\n\u001b[1;32m-> 1186\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_wrapfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'argmax'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1187\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1188\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\pd\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36m_wrapfunc\u001b[1;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mbound\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     62\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m         \u001b[1;31m# A TypeError occurs if the object does have such a method in its\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from time import process_time \n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Start the stopwatch / counter  \n",
    "t1_start = process_time() \n",
    "\n",
    "agent = dynaq.DynaQ(state_size, action_size, env)\n",
    "agent.initialize_state_actions() \n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "    if i_episode % 100 == 0:\n",
    "        print(\"\\rEpisode {}/{}.\".format(i_episode, num_episodes), end=\"\")\n",
    "        sys.stdout.flush()\n",
    "    agent.control()\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "t1_stop = process_time() \n",
    "   \n",
    "print(\"Elapsed time:\", t1_stop, t1_start) \n",
    "print(\"Elapsed time during the whole program in seconds:\", \n",
    "                                         t1_stop-t1_start) \n",
    "\n",
    "DynaQ_Q= agent.Q_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_episodes = 100\n",
    "# env.reset()\n",
    "# rewards =[]\n",
    "# r=0\n",
    "# for e in range(test_episodes):\n",
    "# #     print('***********************{}********************'.format(e))\n",
    "#     done = False\n",
    "#     state = env.reset()\n",
    "#     r=0\n",
    "#     while not done:\n",
    "# #         env.render()\n",
    "# #         print(state)\n",
    "# #         print(agent.Q_vals[state])\n",
    "#         action = np.argmax(DynaQ_Q[state])\n",
    "#         #print(action)\n",
    "#         new_state, reward, done, info = env.step(action)\n",
    "#         r+=reward\n",
    "#         state = new_state\n",
    "#         if done:\n",
    "#             rewards.append(r)\n",
    "# #             print(r)\n",
    "#             break\n",
    "# env.close()    \n",
    "# DynaQ_rewards=rewards\n",
    "# print('the score is: {}'.format(sum(rewards)/test_episodes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.save('model_output/final_DynaQ_50K.pkl')\n",
    "\n",
    "agent.load('model_output/final_DynaQ_50K.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dqn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'Adam' from 'keras.optimizers' (C:\\Users\\smr99\\anaconda3\\envs\\ps\\lib\\site-packages\\keras\\optimizers.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_20248\\3353853806.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0magents\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdqn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Desktop\\PhD\\Git\\Syed_progress\\exercises\\Taxi\\agents\\dqn.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mAdam\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mAdam\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mDQNAgent\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'Adam' from 'keras.optimizers' (C:\\Users\\smr99\\anaconda3\\envs\\ps\\lib\\site-packages\\keras\\optimizers.py)"
     ]
    }
   ],
   "source": [
    "from agents import dqn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size =10\n",
    "n_episodes = 25000\n",
    "output_dir = 'model_output/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 24999/25000. ...0.10002340004680012"
     ]
    }
   ],
   "source": [
    "agent =  dqn.DQNAgent(4,6, env, decode_states,n_episodes,batch_size, output_dir ,False) # initialise agent\n",
    "\n",
    "agent.learning( )\n",
    "\n",
    "agent.save(agent.output_dir + \"final_weights.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "agent.load(agent.output_dir + \"final_weights.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[43m \u001b[0m: |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : :\u001b[43m \u001b[0m: |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : :\u001b[43m \u001b[0m: : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (West)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| :\u001b[43m \u001b[0m: : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (West)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :\u001b[35mG\u001b[0m|\n",
      "| :\u001b[43m \u001b[0m| : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m:\u001b[43m \u001b[0m| : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[34;1m\u001b[43mR\u001b[0m\u001b[0m: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (West)\n",
      "+---------+\n",
      "|\u001b[42mR\u001b[0m: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (Pickup)\n",
      "+---------+\n",
      "|R:\u001b[42m_\u001b[0m| : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (East)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| :\u001b[42m_\u001b[0m| : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| :\u001b[42m_\u001b[0m: : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : :\u001b[42m_\u001b[0m: : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (East)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : :\u001b[42m_\u001b[0m: |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (East)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : :\u001b[42m_\u001b[0m|\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (East)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : :\u001b[42m_\u001b[0m|\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35m\u001b[42mG\u001b[0m\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "the score is: 5.0\n"
     ]
    }
   ],
   "source": [
    "test_episodes = 1\n",
    "env.reset()\n",
    "rewards =[]\n",
    "r=0\n",
    "for e in range(test_episodes):\n",
    "#     print('***********************{}********************'.format(e))\n",
    "    done = False\n",
    "    state = env.reset()\n",
    "    r=0\n",
    "    while not done:\n",
    "        env.render()\n",
    "        s =[]\n",
    "        s.append(  decode_states[state]) \n",
    "        act_values = agent.q_estimator.predict(np.array(s))\n",
    "        action=np.argmax(act_values[0])\n",
    "#         print(action)\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        r+=reward\n",
    "        state = new_state\n",
    "        if done:\n",
    "            rewards.append(r)\n",
    "#             print(r)\n",
    "            break\n",
    "env.close()    \n",
    "DQN_rewards=rewards\n",
    "print('the score is: {}'.format(sum(rewards)/test_episodes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #MC_rewards\n",
    "# #Sarsa_rewards\n",
    "# #QLearning_rewards\n",
    "# #DynaQ_rewards\n",
    "# import matplotlib.pyplot as plt\n",
    "# fig= plt.figure(figsize=(10,6))\n",
    "# ax=fig.add_subplot(111)\n",
    "\n",
    "# # plt.plot(MC_rewards,c='b', marker=\"^\",ls='--',label='MC',fillstyle='none')\n",
    "# plt.plot(QLearning_rewards,c='g', ls='--',label='QLearning')\n",
    "# #plt.plot(Sarsa_rewards,c='k',ls='-',label='Sarsa')\n",
    "# plt.plot(DynaQ_rewards,c='r', ls='-',label='DynaQ')\n",
    "# plt.plot(DQN_rewards,c='b', ls='-',label='DQN')\n",
    "\n",
    "# plt.xlabel('Episodes')\n",
    "# plt.ylabel('Rewards')\n",
    "# plt.legend(loc=2)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#QLearning_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5818006863713039"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.rand()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.49492130687032654"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.uniform(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
